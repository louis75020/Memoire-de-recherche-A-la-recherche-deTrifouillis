\documentclass[french]{article}% la classe du document
\usepackage[utf8]{inputenc}% encodagedes caractères
\usepackage[T1]{fontenc}% encodage de la fonte
\usepackage[a4paper]{geometry}% la gestionde la géométriede la page
\usepackage{amsmath}% pour tous les maths
\usepackage{mathtools}% pour tous les maths
\usepackage{graphicx}% pour la gestion des images
\usepackage{babel}% gestion des langues
\usepackage{ntheorem,thmtools}% pourles théorèmes
\usepackage{hyperref}% les liens hypertextes
\usepackage{biblatex}

\addbibresource{bibliorapport.bib}



\begin{document}	
	
	\newpage
	
	\section*{Introduction}
	
		\subsection*{Motivation}
		
				Un toponyme est un nom de lieu. Le but de notre sujet est de trouver un lien entre les toponymes des communes françaises et leur position géographique. En effet, sans jamais avoir entendu parler d'un certain nombre de communes il est possible d'avoir un à priori sur leur situation géographique. La présence de certaines lettres telles que le w, de certaines syllabes telles que 'ac', ou de certaines sous-chaînes de caractères comme 'sur Somme' semblent indiquer que les toponymes correspondant se trouvent respectivement en Alsace, en Aquitaine et en Picardie.
				
				\begin{figure}
					\centering
					\includegraphics[width=0.7\linewidth]{Anova_rapport_files/figure-latex/unnamed-chunk-7-2}
					\caption[Représentation des communes selon la présence de la lettre k]{Bleu: la commune contient un k}
					\label{k}
				\end{figure}
				
				\begin{figure}
					\centering
					\includegraphics[width=0.7\linewidth]{Anova_rapport_files/figure-latex/unnamed-chunk-7-3}
					\caption[Représentation des communes selon la présence de la lettre w]{Bleu: la commune contient un w}
					\label{w}
				\end{figure}
			
			
				Nous essaierons donc de trouver une procédure automatique pour estimer la région (méthodes de classification) ou les coordonnées (méthodes de régression) des toponymes de communes françaises. 
				
				La problématique est en fait double:
				
				\begin{itemize}
					
					\item Nous devons tout d'abord trouver des façons de transformer des chaînes de caractères (toponymes) en données numériques utilisables pour faire de l'apprentissage statistique.
					
					\item Nous devons aussi essayer différents modèles explicatifs et les comparer pour trouver le meilleur modèle prédictif.
					
				\end{itemize}
				
				Finalement, nous utiliserons les différents modèles crées pour chercher sur la carte de France où se trouve la commune fictive de "Trifouillis-les-oies" afin de constater si oui ou non les différents modèles s'accordent.
				
		\subsection*{Les données}
		
				\begin{figure}
					\centering
					\includegraphics{"Anova_rapport_files/figure-latex/unnamed-chunk-4-1"}
					\caption{Carte de France}
					\label{Représentation des communes Françaises}
				\end{figure}
			
		\tableofcontents
		\newpage
		
	\section{Modèles linéaires}
	
		Il peut paraître surprenant de commencer avec des modèles aussi simples (que ce soit de classification ou de régression) car nos données ne sont ni gaussiennes, ni même asymptotiquement gaussiennes (voir la suite); mais il y a tout de même une raison de commencer dans cette direction. Contrairement aux arbres de décision et aux réseaux de neurones ces modèles sont interprétables et ce qui marche bien ici marchera sûrement encore mieux pour des modèles plus compliqués. De plus, ceci nous donne un objectif de performance en dessous duquel il ne faudrait pas descendre (on veut éviter de complexifier le modèle sans que cela ait un impact positif sur les performances du modèle). Ainsi, les modèles linéaires nous donneront des pistes sur la manière de comprendre et de modéliser les données.
		
	\subsection{Description des covariables}
	
		Tout d'abord nous allons placer les villes sur la carte (en utilisant tout simplement leurs coordonnées).
		
		Consulter \autoref{Représentation des communes Françaises} 
		
		Nous observons 2 choses:
		\begin{itemize}
		
			\item Toutes les villes de la base de données sont situées en France métropolitaine sans la Corse.
		
			\item Les villes ne sont ni uniformément réparties sur la carte ni même concentrées autour d'un point (ce qu'il faudrait pour modéliser par une loi normale). C'est assez représentatif en revanche de la géographie réelle des communes Françaises.
			
		\end{itemize}
		
		Dans cette partie nous allons traiter une chaîne de caractères comme un vecteur numérique de la façon suivante: pour chacun des caractères (majuscules, lettres accentuées, espaces et - inclus) nous allons comprendre un mot comme une suite de modalités représentant le nombre d'itérations des lettres du mot.
		
		Ainsi 'Paris' s'écrira comme un vecteur (de taille 66-26 lettres de l'alphabet, 26 majuscules, -, \_ et 12 caractères accentués) avec 1 aux modalités P,a,r,i,s et 0 ailleurs.
		La ville de Lille aura 1 aux modalités L, i et e et 2 à la modalité l.
		
		Si ceci est une compréhension extrêmement naïve des chaînes de caractères cela a quand même un sens car certaines lettres comme le k ou le w portent beaucoup d'information sur la localisation de la ville juste par leur présence/ absence.
		
		Voir \autoref{k} et \autoref{w}
		
		Enfin en regardant les fréquences d'apparition de chaque lettre on remarque que certaines lettres se démarquent par leur rareté c'est le cas de:  j,k,w,ç,à,ê,ô,ë,ÿ,î,â,û,ü,Z,Y,U,I,Q,K,W,X
		
		\paragraph*{Notations}
		
		On notera dans toute cette section: \begin{itemize}
			
			\item n le nombre de communes de la base d'apprentissage
			\item n' le nombre de communes dans la base de tests
			
			\item $(longitude_{i},latitude_{i})$ les coordonnées de la commune i.
			\item$latitude:=(latitude_{i})_{i=1\dots n}^{T}$
			\item $latitude:=(longitude_{i})_{i=1\dots n}^{T}$
			
			\item $Y_ \in \{latitude,longitude\} $
			\item $\beta$ le régresseur associé à Y (selon que Y désigne la latitude ou la longitude)
			
			\item $\mathbf{L}$ l'ensemble des caractères présents dans les noms
			
			\item $X_{i}=(1,a_{i},b_{i},\dots)$
			\item $X=[X_{i}]_{i=1,\dots,n}$
			\item $X_{i,\alpha}$ nombre d'apparitions de la lettre $\alpha\in\mathbf{L}$ dans le nom de la commune i.
			
		\end{itemize}
		
		
	
	\subsection{Estimation des coordonnées}
	
	\subsubsection{Modèles réguliers}
	
	\paragraph*{Informations sur le modèle} 
		
		Dans cette partie nous allons créer un modèle linéaire.
		
		Nous considérons ici que la latitude et la longitude d'une commune sont des variables indépendantes (et peuvent donc être estimées séparément).
		
		Pour toute commune i de la base de données de coordonnées$(longitude_{i},latitude_{i})$
		
		$longitude_{i}=X_{i}\beta_{1} + \epsilon_{i}$
		
		$latitude_{i}=X_{i}\beta_{2} + \eta_{i}$
		
		et $X_{i}=(1,a_{i},b_{i},\dots)$
		
		Avec $\eta_{i}$ indépendant de $\epsilon_{j}$ pour tout i, j; et $\beta_{1}, \beta_{2} $ inconnus.
		
		On veut $\beta_{1} (resp. \beta_{2}) \in argmin\{||latitude-X\beta_{1}||\} (resp. longitude,\beta_{2})$
		
		Avec $latitude:=(latitude_{i})_{i=1\dots n}^{T}$ et $X=[X_{i}]_{i=1,\dots,n}$
		
		Des estimateurs de $\beta_{1}$ et $\beta_{2}$ sont données par:
		
		$\hat{\beta_{1}}:=(X'X)^{-1}X'longitude$ 
		
		$\hat{\beta_{2}}:=(X'X)^{-1}X'latitude$ 
		
		Pour plus de détails sur ces estimateurs, merci de consulter \cite[chap.1-4]{Cornillons2019} et\cite[ch.1]{Donnet}
		
	\paragraph*{Tests sur les covariables}
		
		Nous allons nous servir de ce modèle pour tester l'impact du nombre d'itérations de chaque lettre sur la latitude et la longitude pour voir quelles lettres auront eu le plus d'impact sur les variables d'intérêt.
		
		Pour cela nous allons faire des tests de Fischer: c'est à dire nous allons regarder si la rapport des variances entre le modèle linéaire comprenant toutes les covariables moins celle que nous testons d'une part et d'autre part le modèle linéaire complet.
		
		Pour $\delta \in \mathbf{L}$ on teste:
		
		$H_{0}:Y_{i}=\beta_{0}+\sum_{\alpha\in L, \alpha \ne \delta}\beta_{\alpha}X_{i,\alpha}$ vs $H_{1}:Y_{i}=\beta_{0}+\sum_{\alpha\in L}\beta_{\alpha}X_{i,\alpha}$ Avec:
		
		$\beta_{\alpha}$ régresseur associé à la lettre $\alpha$
		
		$X_{i,\alpha}$ nombre d'apparitions de la lettre alpha dans le nom de la commune i.
		
		Pour plus de détails sur la statistique de test et sur la zone de rejet, consulter \cite{Donnet}
		
		Il s'agit sous R et SAS de la procédure 'anova 1'. Les résultats de ces tests de Fischer sont décrits ci-dessous:
		
%		\begin{table}[!htbp] 
%			
%			\centering 
%			
%			\begin{tabular}{|c|c|c|c|}
%			
%			\hline
%				\multicolumn{4}{|c|}{Résultats des tests de Fischer}
%			\\
%			\hline
%				P-values:
%				& {0}
%				& {< 5/100} 
%				& {>5/100 } 
%			\\
%			\hline
%				Latitude 
%				& {a z e r y u i o 
%				 p s d f g h j 
%				 k l m v n é è 
%				 \_ ë A Z E U P S 
%				 F H K L W C V B}
%				& {B I J â ü T  }
%				& {t q x c – ç à ê ô ÿ î û R Y O Q D G M X N}
%			\\
%			\hline
%				Longitude
%				& {a e t y u i q s d f h k m w x b n é – è \_ ô ë A Z E R T Y P Q S D G H J L 
%				M W X C V B N }
%				& {l I K o c F}
%				& {z r p g j v ç à ê ÿ î â û ü U O}
%			\\
%			\hline
%			\end{tabular}
%		\end{table}

%		\begin{verbatim}
%			P-values:			0				<5%				>5%	
%					
%						a z e r y u i o 	B I J â ü T		t q x c – ç à 
%						p s d f g h j k						ê ô ÿ î û R Y
%			Latitude:	l m v n é è _ ë 					O Q D G M X N
%						A Z E U P S F H 
%						K L W C V B
%						
%						a e t y u i q s 	l I K o c F		z r p g j v ç
%						d f h k m w x b 					à ê ÿ î â û ü U O
%			Longitude:	n é – è _ ô ë A 
%						Z E R T Y P Q S 
%						D G H J L M W X 
%						C V B N 	
%						
%		\end{verbatim}

%INCLURE CES SORTIES!
		
		\begin{itemize}
			
			\item La plupart des lettres ont une p-value <5\% (c'est à dire que les tests simples sont extrêmement significatifs): le plupart des lettres ont un impact statistiquement significatif sur la latitude. Attention néanmoins aux tests multiples: si on fixe le niveau des tests à 5\% (tests multiples sans ajustement du niveau des tests): on arrive à un niveau global des tests de 100%...
			
			\item Cependant, il y a beaucoup de p-values à 0 c'est à dire que beaucoup de lettres semblent avoir un impact extrêmement significatif sur les coordonnées d'une commune. Cela va compliquer la procédure de sélection des variables. Si on utilise une procédure de type Bonferroni-Holmes; en fait la somme des p-values <5\% 	nous donne 12\% pour la longitude et 2\% pour la latitude donc en fait on peut se permettre de garder le niveau des tests à 5\%.
			
		\end{itemize}

	\paragraph*{Hypothèses du modèle}
	
		Dans les modèles linéaires nous faisons l'hypothèse que les bruits $\epsilon_{i}\sim iid N(0,\sigma^{2}) ; \forall i \in (1,\dots,n)$ . Il y a en fait 4 hypothèses: l'indépendance des bruits, la normalité des bruits, leur moyenne constante et nulle (on ne néglige pas de tendance); et leur variance constante. Nous testerons graphiquement ces hypothèses. Nous accédons au bruit via les résidus: 
		
		$Y=X\beta +\epsilon; \hat{Y}=X\hat{\beta}$; donc 
		
		$\epsilon\sim Y-\hat{Y}$ avec $Y \in \{latitude,longitude\}; \beta \in \{\beta_{1},\beta_{2}\}$
	
		Les graphes de diagnostiques sont en annexes: \autoref{d1} pour la latitude, \autoref{d2} pour la longitude.
		
		\begin{itemize}
		
			\item\textit{Residuals vs fitted}: Ces graphiques nous permettent de tester si les résidus sont de moyenne nulle. Nous ne sommes pas sensés observer de tendance particulière mais ici, nous voyons bien une tendance (décroissante) pour la longitude comme pour la latitude; l’espérance des résidus n’est pas nulle.
			
			\item \textit{QQ-plot}: Ces graphiques nous permettent de tester la normalité de la latitude et de la longitude. Si ces variables suivaient une loi normale nous devrions observer des droites ce qui n'est pas le cas.
			
			\item \textit{Scale Location}: Sous l'hypothèse d'homoscédasticité (variances des résidus constantes) nous ne devrions pas observer de tendance particulière sur ce graphe. Ce n'est pas le cas en particulier pour la longitude.
			
			\item \textit{Residuals vs leverage} :Quelques points sont très loin sur la droite. Si aucun point ne semble aberrant il y a l’air d’y avoir un effet de classe sur les covariables. En regardant les points extrêmes on se rend compte que les noms des communes comprennent de nombreux caractères rares: il est possible que ces villes soient seules dans leurs combinaisons de modalités.
			
		\end{itemize}
	
		Nous observons finalement qu'aucune des hypothèses du modèle linéaire n'est vérifiée. Pour tenter d'améliorer le modèle (le faire se conformer à ces hypothèses) nous avons essayé d'appliquer les fonctions $log(|.|)$ et $\sqrt(|.|)$ à la latitude et la longitude sans que cela ait amélioré ces hypothèses (les graphes de diagnostique correspondant sont en annexe \autoref{d3}, \autoref{d4}). On en déduit que les coordonnées des communes ne dépendent pas linéairement des covariables choisies.
	
	\paragraph*{Sélection des variables}
		
		Il convient maintenant de sélectionner les lettres ayant le plus d'impact sur les coordonnées des communes françaises. Pour cela nous avons déjà fait la procédure de type anova1. Nous allons tester 2 autres procédures: AIC backward et BIC backward. En d'autres termes pour sélectionner les covariables pertinentes nous allons appliquer l'algorithme suivant:
		
		(Initialisation): On part du modèle plein; comprenant l'ensemble des covariables. On choisit de plus une fonction f d'utilité BIC ou AIC.
		
		(Itération k): On a sélectionné $M_{k}$ modèle à k variables et on crée l'ensemble des modèles à k-1 variables; et on calcule l'utilité de ces modèles: puis on prend le modèle $M_{k-1}$ à k-1 variables ayant maximisé l'utilité. 
		Si $f(M_{k})\ge f(M_{k-1})$ on s'arrête et on garde le modèle $M_{k}$
		Sinon on itère avec le modèle $M_{k-1}$.
		
		$AIC(\xi)->nlog\frac{SCR(\xi)}{n}+n(1+log2\pi)+2|\xi+1|$
		
		$BIC(\xi)->n(1+log2\pi)+nlog\frac{SCR(\xi)}{n}+|\xi+1|log (n)$
		
		$n$ le nombre de variables
		
		$|\xi$| nombre de variables explicatives sélectionnées
		
		$SCR(\xi):=||Y-X\xi||$
		
		Nous avons finalement crée 4 modèles: le modèle linéaire complet; le modèle après anova1, le modèle après AIC et le modèle après BIC. Voyons leurs performances (sur la base de tests):
		
		\begin{verbatim}
		##                          Erreur sur la latitude Erreur sur la longitude
		## Modele complet                      0.001217323             0.001861603
		## Modele avec anova type 1            0.001224906             0.001881928
		## Modele AIC backward                 0.001218195             0.001862060
		## Modele BIC backward                 0.001218928             0.001864996
		## Modele trivial                      0.001461842             0.002055721
		\end{verbatim}
		
		On préférera finalement le modèle BIC.
		
		\paragraph*{Elastic-net}
			
			Pour sélectionner des variables et pour éviter le problème des coefficients proches de 0 mais dont les tests de Student nous font rejeter l'hyphothèse de nullité des coefficients nous avons décidé de finalement créer un dernier modèle de type Elastic-net. Il s'agit en fait des mêmes modèles $Y_{i}=X_{i}\beta + \epsilon_{i}$ mais nous rajoutons une contrainte sur $\beta$ du type:
			
			$\beta\in argmin\{||X\beta-Y||_{2}^{2}+\lambda||\beta||_{2}+\mu||\beta||_{1}\}$ pour$\lambda,\mu>0$ 
			(le ML simple correspondait au cas $\mu=\lambda=0$).
			
			Pour choisir efficacement $\lambda$ et $\mu$ nous allons appliquer la procédure de validation croisée. Pour plus de détails, consulter \cite{Cornillon2019}.
			
			On voit que les paramètres ont bien convergé (cf \autoref{e1},\autoref{e2}) pour toutes les covariables. Néanmoins lorsque nous calculons l'erreur quadratique de prévision par le modèle Elastic-net et que nous le comparons aux autres erreurs quadratiques Elastic-net fait finalement aussi bien que les modèles linéaires.	
		
	\subsubsection{Modèles singuliers}
	
		\paragraph*{Informations sur le modèle} 
		
			Dans cette partie nous allons rajouter un niveau d'interaction dans le modèle (nous intéresser en plus à la présence/absence d'une seule lettre nous allons aussi regarder la présence simultanée de 2 lettres).
			
%			En notant $\mathbf{L}$ l'ensemble des caractères présents dans les noms; Pour Y latitude ou longitude:
%			
%			$Y_{i}=\beta_{0}+\sum_{\alpha,\gamma \in \mathbf{L}}\beta_{\alpha,\gamma}X_{i,\alpha,\gamma} + \epsilon_{i}$; $\forall i \{1,\dots,n\}$ 
%			
%			$\epsilon_{i}\sim iid N(0,\sigma^{2})$
			
			%et $X_{i}$ vecteur de taille $card(\mathbf{L})$ comprenant à l'indice i le nombre d'apparitions de la lettre $\alpha_{i}$
			
			L'ANOVA dans le cadre du modèle singulier n'est pas un modèle facile à expliciter. Néanmoins nous allons donner 2 exemples pour montrer de quoi il s'agit en pratique.
			
			Pour la latitude de Dax cela donne:
			
			$latitude_{Dax}=\beta_{0}+\beta_{D}+\beta_{a}+\beta_{x}+\beta_{D}+\beta_{D}+\beta_{a}+\epsilon_{Dax}$
			
			Pour la longitude de Lille cela donne:
			
			$longitude_{Lille}=\beta_{0}+\beta_{L}+\beta_{i}+\beta_{2l}+\beta_{e}+\beta_{L,i}+\beta_{L,2l}+\beta_{L,e}+\beta_{i,2l}+\beta_{i,e}+\beta_{2l,e}+\eta_{Lille}$
			
			Pour plus de détails sur la forme des estimateurs; consulter \cite{Donnet}
			
			
		\paragraph*{Hypothèses du modèle et sorties}
		
			Nous allons vérifier si les hypothèses du modèle (qui sont formellement les mêmes que pour le modèle régulier) sont vérifiées.
			
			Ces graphiques sont aux annexes: \autoref{d5}, \autoref{d6}.
			
			\begin{itemize}
				
				\item \textit{Residuals vs fitted}: la tendance observée dans le cas régulier est encore visible.
				
				\item \textit{Normal Q-Q}: Le QQ plot ressemble presque à une droite, le fait de rajouter des interactions a permis d'applatir la droite.
				
				\item \textit{Scale-location}: La tendance qui était percevable dans le modèle régulier est nettement moins visible.
				
				\item R\textit{esiduals vs leverage}: il y a moins d'outliers que dans le modèle régulier.
				
			\end{itemize} 
		
			Les hypothèses du modèle linéaire ne sont toujours pas graphiquement validées ceci dit on est maintenant beaucoup plus tenté de les accepter dans le cas du modèle singulier. On est tentés de dire que si on pouvait ajouter autant d'interactions qu'on voudrait sans prendre en compte les temps de calcul on pourrait finir par accepter l'hypothèse selon laquelle les coordonnées d'une ville suivent des lois normales. Nous allons maintenant commenter les sorties du modèle:
			
			\begin{itemize}
				
				\item A propos des régresseurs: On remarque qu’il y a beaucoup de régresseurs ps non nuls (c’est à dire avec des tests de nullité avec une p-value au moins <5\%) le modèle est donc très gros. La sélection des variables risque aussi d’être compliquée à cause de la taille des modèles.
				
				\item A propos des $R^{2}$ on obtient les scores 18\% pour la longitude et 20\% pour la latitude. Ce serait assez faible dans le cadre modèle gaussien mais c’est très intéressant pour notre sujet de remarquer qu’avec des modèles simples et faux on arrive quand même à expliquer environ 1/5 de la variabilité des positions des villes juste à partir de leur nom.
				
				\item A propos de la statistique de Fischer: on obtient une p-value de pertinence du modèle proche de 0 c’est à dire que ps la présence d’au moins un des caractères étudiés é un impact sur la latitude (resp. longitude) des villes en France: c’est très encourageant!
				
				\item A propos des NA: Ils viennent sans doute du fait que nous avons considéré des interactions qui n’existent pas. Il est possible qu’aucune ville n’ait à la fois les lettres a et ü dans leu nom (les NA sont les mêmes pour la latitude et la longitude). Il serait pertinent de ne pas regarder alors ces interactions…
				
			\end{itemize}	
			
		\paragraph*{Sélection des variables}
		
			Pour la sélection des variables avec 1 niveau d'interactions on a plus de 2000 coefficients à tester il n'y a donc pas moyen de faire BIC ou AIC, même avec une procédure car les temps de calcul deviennent énormes. En effet avec une procédure forward (par exemple): à l'itération 1000 nous devrions créer 1000 modèles à 1001 coefficients... De plus comme beaucoup (plus de 200) de ces coefficients sont presque sûrement non nuls il y a de fortes chances pour que nous ayons effectivement à considérer ces modèles. Pour la sélection des variables nous allons utiliser encore une fois une procédure de type anova1.
		
			Nous allons donc tester l'influence de chacune des covariables ainsi que leurs interactions sur sur la variance des coordonnées. Pour visualiser ces tests nous allons garder l'exemple de la ville de Dax.
			
			Pour la 1ère série de tests il s'agit des tests de Fischer ou nous testons la pertinence du modèle sans inclure une covariable (ici a par exemple a) contre le modèle complet.
			
			$H_{0}:Y_{Dax}=\beta_{0}+\beta_{D}+\beta_{x}+\beta_{D,x}+\epsilon_{Dax}$ vs $H_{1}:Y_{Dax}=\beta_{0}+\beta_{D}+\beta_{a}+\beta_{x}+\beta_{D,a}+\beta_{D,x}+\beta_{a,x}+\epsilon_{Dax}$
			
%			Pour $\delta \in \mathbf{L}$ on teste:
%			
%			$H_{0}: Y_{i}=\beta_{0}+\sum_{\alpha,\gamma \in \mathbf{L/\delta}}\beta_{\alpha,\gamma}X_{i,\alpha,\gamma} + \epsilon_{i}$; vs $H_{1}:Y_{i}=\beta_{0}+\sum_{\alpha,\gamma \in \mathbf{L}}\beta_{\alpha,\gamma}X_{i,\alpha,\gamma} + \epsilon_{i}$; $\forall i \{1,\dots,n\}$
%			
%			En d'autres termes on fait un rapport de variances entre le modèle moins la variable  (ici $\delta$)que l'on cherche à tester et ses interactions avec les autres variables; et le modèle complet.

			Pour la seconde série (si on a rejeté $H_{0}$ dans le 1er test); on s'intéressera à la pertinence de ses interactions avec les autre variables (pour Dax mettons qu'on garde le a à l'issue du 1er test et que nous nous interrogeons sur la pertinence de l'interaction a:x) on testera:
			
			$H_{0}:Y_{Dax}=\beta_{0}+\beta_{D}+\beta_{a}+\beta_{x}+\beta_{D,a}+\beta_{D,x}+\epsilon_{Dax}$ vs $H_{1}:Y_{Dax}=\beta_{0}+\beta_{D}+\beta_{a}+\beta_{x}+\beta_{D,a}+\beta_{D,x}+\beta_{a,x}+\epsilon_{Dax}$
			
%			Pour la seconde série il s'agit de: 
%			
%			Pour $(\delta_{1},\delta_{2}) \in \mathbf{L}^{2}$ on teste:
%			
%			 $H_{0}: Y_{i}=\beta_{0}+\sum_{(\alpha,\gamma) \in \mathbf{L}^{2}/(\delta_{1},\delta_{2})}\beta_{\alpha,\gamma}X_{i,\alpha,\gamma} + \epsilon_{i}$;  vs $H_{1}:Y_{i}=\beta_{0}+\sum_{\alpha,\gamma \in \mathbf{L}}\beta_{\alpha,\gamma}X_{i,\alpha,\gamma} + \epsilon_{i}$; $\forall i \{1,\dots,n\}$
%			
%			Ou l'on teste le modèle complet sans le terme d'interaction $(\delta_{1},\delta_{2}), \delta_{1} \ne \delta_{2}$ contre le modèle complet.		

			La statistique de tests, ainsi que l'ensemble de la procédure anova 1 sont décrits dans la section 'tests statistiques des différents effets' du chap.3 de \cite{Donnet}
			
			Nous fixons le niveau de risque à 5\% et nous réduisons ainsi les modèles à environ 400 paramètres pour chacune des coordonnées.
	
%	\subsubsection{Modèle MARS}
%	
%		\paragraph*{Information sur le modèle}
%				
%		
%			Dans cette partie, nous allons tenter un modèle MARS (Multivariate adaptative regression splines) qui est une méthode de régression non paramétrique qui est une forme de modèle linéaire par morceaux. Ce modèle nous permettra d'une part d'estimer les coordonnées jointement et donc de relâcher l'hypothèse d'indépendance entre les coordonnées des communes et d'autre part de traiter la non-linéarité des coordonnées.
%			
%			$\hat{Y}_{i}=\sum_{j=1}^{k}c_{j}B_{j}{X_{i}}$
%			
%			Avec: $Y_{i}$ latitude ou longitude de la commune i; les $c_{j}$ coefficients constants et les $B_{j}$ sont des fonctions splines ie de la forme:
%			$
%			 
%		
%		\paragraph*{Diagnostique du modèle}
	
	\subsubsection{Conclusions}
	
		\paragraph*{Performances des modèles}
		
			Nous avons crée nos modèles en utilisant une base d'apprentissage comprenant 90\% de la base de données. Nous nous servons maintenant des 10\% des communes restantes rassemblées dans une base de tests pour calculer l'erreur (quadratique) d'apprentissage. Pour chacun des modèles nous calculons:
			
			$\frac{1}{n'}\sum_{i=1}^{n'}((longitude_{i}-\hat{longitude_{i}})^{2},(latitude_{i}-\hat{latitude_{i}}^{2}))$
			
			Et nous regardons le modèle minimisant la moyenne des erreurs quadratiques pour voir lequel de nos modèles prédit le mieux.
			
			\begin{verbatim}
			##                                 Erreur sur la latitude Erreur sur la longitude
			## Modele complet                             0.001217323             0.001861603
			## Modele avec anova type 1                   0.001224906             0.001881928
			## Modele AIC backward                        0.001218195             0.001862060
			## Modele BIC backward                        0.001218928             0.001864996
			## Modele trivial                             0.001461842             0.002055721
			## Modele singulier                           0.007005175             0.004707507
			## Modele singulier anova de type1            0.001136769             0.001788715
			## Modele Lasso avec lambda opt               0.001217612             0.001861504
			\end{verbatim}
			
			Le meilleur modèle que nous ayons crée est le modèle singulier après sélection des variables via l'anova de type 1.
			
		\paragraph*{Sélection des individus}
		
			Il est maintenant intéressant de voir pour le meilleur modèle quelles villes ont été les mieux placées. Nous allons regarder pour la latitude seulement; puis pour la longitude seulement et enfin globalement, voici les sorties globales:
			
			\begin{verbatim}
			## # A tibble: 20 x 6
			##    nom                        code_postal longitude latitude departement region
			##  1 Bouville                   91880          0.0399    0.845          91 IF    
			##  2 Bischwihr                  68320          0.130     0.839          68 AL    
			##  3 Villemer                   89113          0.0606    0.836          89 BO    
			##  4 Béthemont-la-Forêt         95840          0.0393    0.856          95 IF    
			##  5 La_Saunière                23000          0.0338    0.805          23 LI    
			##  6 Orly-sur-Morin             77750          0.0564    0.854          77 IF    
			##  7 Sementron                  89560          0.0586    0.832          89 BO    
			##  8 Neewiller-près-Lauterbourg 67630          0.142     0.854          67 AL    
			##  9 Mainvilliers               45330          0.0398    0.843          45 CE    
			## 10 Archignat                  3380           0.0423    0.809           3 AU 
			\end{verbatim}
			
			Nous observons que pour l'ensemble des villes les mieux placées, peu importe le critère; il n’y a pas spécialement de points communsentre ces villes. Elles ne sont pas dans la même région ni proches du centre. En revanche leurs noms rappellent le nom des villes ayant l’air d’avoir eu un fort leverage: leurs coordonnées sont sans doutes estimées par elles-mêmes. Ces villes ont peut-être une combinaison de modalités unique.
			
	\subsection{Modèles linéaires généralisés et classification}

	\subsubsection{Régressions logistiques binaires}

	\subsubsection{Régression logistique multinomiale}	
	
	\section{Arbres de décision}
	
	\subsection{Forêts aléatoires}
	
	\subsubsection{Classification avec les données des régressions logistiques}
	
	\subsection{Gradient Boosté}
	
	\section{Réseaux de neurones}
	
	
		
	\newpage
	
	\printbibliography
	
	\newpage
	
%	\section*{Annexes}
	
%
%	\begin{figure}
%		\centering
%		\includegraphics[width=0.7\linewidth]{Anova_rapport_files/figure-latex/unnamed-chunk-7-2}
%		\caption[Représentation des communes selon la présence de la lettre k]{Bleu: la commune contient un k}
%		\label{k}
%	\end{figure}
%	
%	\begin{figure}
%		\centering
%		\includegraphics[width=0.7\linewidth]{Anova_rapport_files/figure-latex/unnamed-chunk-7-3}
%		\caption[Représentation des communes selon la présence de la lettre w]{Bleu: la commune contient un w}
%		\label{w}
%	\end{figure}
%
%	
%
%	\begin{figure}
%		\centering
%		\includegraphics{Anova_rapport_files/figure-latex/unnamed-chunk-11-1}
%		\caption[Diagnostique de la latitude du modèle régulier]{Diagnostique de la latitude modèle régulier}
%		\label{d1}
%	\end{figure}
%	
%	\begin{figure}
%		\centering
%		\includegraphics{Anova_rapport_files/figure-latex/unnamed-chunk-13-1}
%		\caption[Diagnostique de la longitude du modèle régulier]{Diagnostique de la longitude du modèle régulier}
%		\label{d2}
%	\end{figure}
%
%	\newpage
%	
%	\begin{figure}
%		\centering
%		\includegraphics[width=0.7\linewidth]{Anova_rapport_files/figure-latex/unnamed-chunk-16-1}
%		\caption[Diagnostique de la log-latitude]{Diagnostique de la log-latitude}
%		\label{d3}
%	\end{figure}
%	
%	\begin{figure}
%		\centering
%		\includegraphics[width=0.7\linewidth]{Anova_rapport_files/figure-latex/unnamed-chunk-16-2}
%		\caption[Diagnostique de la log-longitude]{Diagnostique de la log-longitude}
%		\label{d4}
%	\end{figure}
%
%	\begin{figure}
%		\centering
%		\includegraphics{Anova_rapport_files/figure-latex/unnamed-chunk-27-1}
%		\caption[Diagnostique de la latitude du modèle singulier]{Diagnostique de la latitude du modèle singulier}
%		\label{d5}
%	\end{figure}
%
%	\begin{figure}
%		\centering
%		\includegraphics{Anova_rapport_files/figure-latex/unnamed-chunk-27-2}
%		\caption[Diagnostique de la longitude du modèle singulier]{Diagnostique de la longitude du modèle singulier}
%		\label{d6}
%	\end{figure}
%	
%	\begin{figure}
%		\centering
%		\includegraphics{Anova_rapport_files/figure-latex/unnamed-chunk-35-1}
%		\caption[Elastic-net latitude]{Elastic-net latitude}
%		\label{e1}
%	\end{figure}
%	
%	\begin{figure}
%		\centering
%		\includegraphics{Anova_rapport_files/figure-latex/unnamed-chunk-35-2}
%		\caption[Elastic-net longitude]{Elastic-net longitude}
%		\label{e2}
%	\end{figure}
	
%	\newpage
%	\listoffigures	
		
	
	\end{document}